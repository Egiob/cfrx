{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f5741ac-1f54-4d36-9a42-51aa548255e4",
   "metadata": {},
   "source": [
    "# Monte Carlo Counterfactual Regret Minimization (MCCFR)\n",
    "\n",
    "In this example we showcase how to use `cfrx` to run the MCCFR (outcome-sampling variation) on simple games Kuhn Poker, Leduc Poker.\n",
    "\n",
    "We'll see how to:\n",
    " - Initialize an environment from `cfrx`\n",
    " - Initialize a random policy and sample a rollout\n",
    " - Write a small training loop to run the MCCFR algorithm\n",
    " - Measure the evolution of our strategy exploitability throughout the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c800a53a-74bf-45a0-9303-813fcf378753",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfea731-df06-4bbd-a829-69ecd2246294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "\n",
    "jax.config.update(\"jax_platform_name\", \"cpu\")\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from cfrx.algorithms.mccfr.outcome_sampling import MCCFRState, do_iteration, unroll\n",
    "from cfrx.metrics import exploitability\n",
    "from cfrx.policy import TabularPolicy\n",
    "from cfrx.utils import regret_matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af634f23-50a4-4913-a5e1-fec14c9e5c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "def plot_partial(plot_fn, *plot_args):\n",
    "    clear_output(wait=True)\n",
    "    fig = plot_fn(*plot_args)\n",
    "    plt.show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6482526b-39c5-48b5-b51d-7b02e28a70f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = jax.devices(\"cpu\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea5d36e-843b-44dd-afa2-952026a1ed0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "ENV_NAME = \"Kuhn Poker\"\n",
    "NUM_ITERATIONS = 100000\n",
    "EXPLORATION_FACTOR = 0.6\n",
    "SEED = 0\n",
    "METRICS_PERIOD = 10000\n",
    "\n",
    "random_key = jax.random.PRNGKey(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915ed59a-954a-4fc9-bbc7-4e2297b2ba2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENV_NAME == \"Kuhn Poker\":\n",
    "    from cfrx.envs.kuhn_poker.env import KuhnPoker\n",
    "\n",
    "    env_cls = KuhnPoker\n",
    "\n",
    "\n",
    "elif ENV_NAME == \"Leduc Poker\":\n",
    "    from cfrx.envs.leduc_poker.env import LeducPoker\n",
    "\n",
    "    env_cls = LeducPoker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f249f61-09bb-4979-9862-5965eb8ca722",
   "metadata": {},
   "source": [
    "## The environment\n",
    "\n",
    "[Kuhn Poker](https://en.wikipedia.org/wiki/Kuhn_poker) is a simplified version of the Poker game. In cfrx, we use the environment from [pgx](https://github.com/sotetsuk/pgx), and add a wrapper to explicitly handle random nodes and information states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0f79d3-84c7-4ceb-bc13-5dcda410366e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = env_cls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bde9d05-f877-4d4d-abd3-7e188c719bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of info_sets, number of possible actions\n",
    "n_states = env.n_info_states\n",
    "n_actions = env.n_actions\n",
    "\n",
    "n_states, n_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e713d110-b39b-444b-9cab-8fddb492c7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = env.init(random_key)\n",
    "s0  # Cards haven't been dealed yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1df2ca-3682-48b0-9536-1e7da6b2e6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give a J to player 1 and a K to player 2\n",
    "s1 = env.step(s0, action=jnp.array(0))\n",
    "s2 = env.step(s1, action=jnp.array(2))\n",
    "jax.tree_map(lambda *z: jnp.stack(z), s1, s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b24419-54ff-4df8-b298-35d66e71ad7e",
   "metadata": {},
   "source": [
    "## Random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b947e034-a996-4fd9-827f-da808e50addd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a training state\n",
    "training_state = MCCFRState.init(n_states, n_actions)\n",
    "jax.tree_map(np.shape, training_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415c8291-e932-44e1-ae23-c7a4a5ae87b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a Policy object and print the probability distribution for our current strategy and state\n",
    "policy = TabularPolicy(\n",
    "    n_actions=n_actions,\n",
    "    exploration_factor=EXPLORATION_FACTOR,\n",
    "    info_state_idx_fn=env.info_state_idx,\n",
    ")\n",
    "\n",
    "policy.prob_distribution(\n",
    "    params=training_state.probs,\n",
    "    info_state=s2.info_state,\n",
    "    action_mask=s2.legal_action_mask,\n",
    "    use_behavior_policy=jnp.bool_(False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccd00e0-87e3-4642-94c2-505bde9971e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do an unroll with our uniformly-random Policy\n",
    "random_key, subkey = jax.random.split(random_key)\n",
    "episode, states = unroll(\n",
    "    init_state=s2,\n",
    "    training_state=training_state,\n",
    "    random_key=subkey,\n",
    "    update_player=0,\n",
    "    env=env,\n",
    "    policy=policy,\n",
    "    n_max_steps=env.max_episode_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928d3ab8-45ae-413e-a6c6-c7f7b85d802d",
   "metadata": {},
   "source": [
    "Print out the action sequence \"b\" means \"bet\" and \"p\" pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37371212-4c6f-4056-88be-a1b053ccdf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.tree_map(lambda x: x[~states.terminated], states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19417467-869d-4602-a4b9-30efc84d3e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\".join(\n",
    "    [env_cls.action_to_string(x) for x in episode.action[episode.mask.astype(bool)]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793e3d82-16c0-4230-94e0-54d6e928427d",
   "metadata": {},
   "source": [
    "## MCCFR implementation\n",
    "We use the `cfrx` components to implement the MCCFR algorithm.\n",
    "\n",
    "The algorithm consists in alternating iterations for the two players, and logging the exploitability from time to time.\n",
    "\n",
    "Note: We make sure to Jit both the iteration and exploitability function, to make the most of Jax capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef1dd9d-ae46-4f0f-b01e-e17c35ee5b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function samples a trajectory, compute counterfactual regrets and update the policy accordingly\n",
    "do_iteration_fn = jax.jit(\n",
    "    lambda training_state, random_key, update_player: do_iteration(\n",
    "        training_state=training_state,\n",
    "        random_key=random_key,\n",
    "        env=env,\n",
    "        policy=policy,\n",
    "        update_player=update_player,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f6c28e-d31d-4c4c-840a-788d7557c6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function measures the exploitability of a strategy\n",
    "exploitability_fn = jax.jit(\n",
    "    lambda policy_params: exploitability(\n",
    "        policy_params=policy_params,\n",
    "        env=env,\n",
    "        n_players=env.n_players,\n",
    "        n_max_nodes=env.max_nodes,\n",
    "        policy=policy,\n",
    "    ),\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb10157b-6127-48a0-8470-708b7d9a7a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One iteration consists in updating the policy for both players\n",
    "n_loops = 2 * NUM_ITERATIONS\n",
    "\n",
    "exploitabilities = []\n",
    "iterations = []\n",
    "\n",
    "for k in range(n_loops):\n",
    "    random_key, subkey = jax.random.split(random_key)\n",
    "\n",
    "    # Update players alternatively\n",
    "    update_player = k % 2\n",
    "    training_state = do_iteration_fn(\n",
    "        training_state=training_state,\n",
    "        random_key=subkey,\n",
    "        update_player=update_player,\n",
    "    )\n",
    "\n",
    "    # Logging\n",
    "    if k == 0 or (k + 1) % (METRICS_PERIOD * 2) == 0:\n",
    "        current_policy = training_state.avg_probs\n",
    "        current_policy /= training_state.avg_probs.sum(axis=-1, keepdims=True)\n",
    "\n",
    "        exp = exploitability_fn(policy_params=current_policy)\n",
    "\n",
    "        exploitabilities.append(exp)\n",
    "        iterations.append(k // 2)\n",
    "        plt.xlabel(\"Iterations\")\n",
    "        plt.title(f\"MCCFR outcome sampling on {ENV_NAME}\")\n",
    "        plt.ylabel(\"Exploitability\")\n",
    "        plt.yscale(\"log\")\n",
    "        plt.xlim(0, NUM_ITERATIONS)\n",
    "\n",
    "        plot_partial(plt.plot, iterations, exploitabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2838874f-cbeb-4de2-94ec-889536c1250c",
   "metadata": {},
   "source": [
    "All this logic is also implemented inside a trainer, which is further optimized to reduce the runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354745a6-e9d1-45d6-b1fe-5663ea728086",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cfrx.trainers.mccfr import MCCFRTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6c2725-f329-40dd-aa48-7c35410a364c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = MCCFRTrainer(env=env, policy=policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d087527c-399e-438f-a153-a0242bb6e93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_state = trainer.train(\n",
    "    random_key=random_key, n_iterations=NUM_ITERATIONS, metrics_period=METRICS_PERIOD\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a1a2d8-c8da-41c4-ab42-6c50bb2a82db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
